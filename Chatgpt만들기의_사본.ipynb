{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomdoori/user1/blob/main/Chatgpt%EB%A7%8C%EB%93%A4%EA%B8%B0%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsxhb4RzJ7MQ"
      },
      "source": [
        "##step0) Colab í™˜ê²½ ì„¤ì •\n",
        "ì„¤ì¹˜(python>=3.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K98IBA2FEuWG",
        "outputId": "34be38ff-a950-4f0b-8c09-f278569e7a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj7Xp2LBJ3cF",
        "outputId": "db3fe977-1d4d-4cf4-e18a-c068f66868d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colossalai==0.2.7\n",
            "  Downloading colossalai-0.2.7.tar.gz (686 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m686.7/686.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (23.1)\n",
            "Collecting pre-commit (from colossalai==0.2.7)\n",
            "  Downloading pre_commit-3.3.2-py2.py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.3)\n",
            "Collecting fabric (from colossalai==0.2.7)\n",
            "  Downloading fabric-3.0.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contexttimer (from colossalai==0.2.7)\n",
            "  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ninja (from colossalai==0.2.7)\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (2.0.1+cu118)\n",
            "Collecting invoke>=2.0 (from fabric->colossalai==0.2.7)\n",
            "  Downloading invoke-2.1.2-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting paramiko>=2.4 (from fabric->colossalai==0.2.7)\n",
            "  Downloading paramiko-3.1.0-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cfgv>=2.0.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading identify-2.5.24-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->colossalai==0.2.7)\n",
            "  Downloading virtualenv-20.23.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->colossalai==0.2.7) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->colossalai==0.2.7) (16.0.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (67.7.2)\n",
            "Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric->colossalai==0.2.7)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (40.0.2)\n",
            "Collecting pynacl>=1.5 (from paramiko>=2.4->fabric->colossalai==0.2.7)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.6 (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7)\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->colossalai==0.2.7) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->colossalai==0.2.7) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
            "Building wheels for collected packages: colossalai, contexttimer\n",
            "  Building wheel for colossalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colossalai: filename=colossalai-0.2.7-py3-none-any.whl size=896479 sha256=39ab706cd8c3917b2ddd5062b18366d08b4eba06af728cf58dace919aa8d01f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/85/25/32a3af943ea5ca261b1b51dae74a4629599ce1bc6fe58dbbfc\n",
            "  Building wheel for contexttimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5803 sha256=2f4d232f5b921506be4d373793558ceca640b5ac2e1e4d53e109a6174bab1097\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4\n",
            "Successfully built colossalai contexttimer\n",
            "Installing collected packages: ninja, distlib, contexttimer, virtualenv, nodeenv, invoke, identify, cfgv, bcrypt, pynacl, pre-commit, paramiko, fabric, colossalai\n",
            "Successfully installed bcrypt-4.0.1 cfgv-3.3.1 colossalai-0.2.7 contexttimer-0.3.3 distlib-0.3.6 fabric-3.0.1 identify-2.5.24 invoke-2.1.2 ninja-1.11.1 nodeenv-1.8.0 paramiko-3.1.0 pre-commit-3.3.2 pynacl-1.5.0 virtualenv-20.23.0\n",
            "/content/drive/MyDrive/colossalai_ChatGPT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/drive/MyDrive/colossalai_ChatGPT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers>=4.20.1 (from chatgpt==0.1.0)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.65.0)\n",
            "Collecting datasets (from chatgpt==0.1.0)\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loralib (from chatgpt==0.1.0)\n",
            "  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: colossalai>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.2.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.0.1+cu118)\n",
            "Collecting langchain (from chatgpt==0.1.0)\n",
            "  Downloading langchain-0.0.177-py3-none-any.whl (877 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m877.7/877.7 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (23.1)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.3)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.0.1)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.20.1->chatgpt==0.1.0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.20.1->chatgpt==0.1.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->chatgpt==0.1.0)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (1.5.3)\n",
            "Collecting xxhash (from datasets->chatgpt==0.1.0)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->chatgpt==0.1.0)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2023.4.0)\n",
            "Collecting aiohttp (from datasets->chatgpt==0.1.0)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from datasets->chatgpt==0.1.0)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (2.0.10)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain->chatgpt==0.1.0)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain->chatgpt==0.1.0)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain->chatgpt==0.1.0)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.10.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->chatgpt==0.1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->chatgpt==0.1.0) (16.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->chatgpt==0.1.0)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->chatgpt==0.1.0)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->chatgpt==0.1.0)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->chatgpt==0.1.0)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->chatgpt==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->chatgpt==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2022.7.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.1)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.24)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.23.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.14.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->chatgpt==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (67.7.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (40.0.2)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.6)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n",
            "Building wheels for collected packages: chatgpt\n",
            "  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46647 sha256=0eebbaa970bf14c4e0b3e99b8a0aa0ede5104f3871fff293d39ff25948e0d4e1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lmsxxy2v/wheels/82/60/2d/65e01194dc6b7aa80256fd50b52dc3260084a2ce14ba7de365\n",
            "Successfully built chatgpt\n",
            "Installing collected packages: tokenizers, xxhash, mypy-extensions, multidict, marshmallow, loralib, frozenlist, dill, async-timeout, yarl, typing-inspect, responses, openapi-schema-pydantic, multiprocess, marshmallow-enum, huggingface-hub, aiosignal, transformers, dataclasses-json, aiohttp, langchain, datasets, chatgpt\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 chatgpt-0.1.0 dataclasses-json-0.5.7 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 langchain-0.0.177 loralib-0.1.1 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 responses-0.18.0 tokenizers-0.13.3 transformers-4.29.2 typing-inspect-0.8.0 xxhash-3.2.0 yarl-1.9.2\n",
            "/content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain==0.0.113\n",
            "  Downloading langchain-0.0.113-py3-none-any.whl (396 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m396.0/396.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0)\n",
            "Collecting SQLAlchemy<2,>=1 (from langchain==0.0.113)\n",
            "  Downloading SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.8.4)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (0.5.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.22.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n",
            "Installing collected packages: SQLAlchemy, langchain\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.10\n",
            "    Uninstalling SQLAlchemy-2.0.10:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.10\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.177\n",
            "    Uninstalling langchain-0.0.177:\n",
            "      Successfully uninstalled langchain-0.0.177\n",
            "Successfully installed SQLAlchemy-1.4.48 langchain-0.0.113\n",
            "/bin/bash: =1.4.1: Operation not supported\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.28.0\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.29.2\n",
            "    Uninstalling transformers-4.29.2:\n",
            "      Successfully uninstalled transformers-4.29.2\n",
            "Successfully installed transformers-4.28.0\n"
          ]
        }
      ],
      "source": [
        "## setup(1min)\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "\n",
        "# setup data\n",
        "\n",
        "%cd /content/drive/MyDrive/colossalai_ChatGPT/\n",
        "!pip install .\n",
        "%cd ../../\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1\n",
        "!pip install transformers==4.28.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fti52YUFK_Gn"
      },
      "source": [
        "##Step 1) SFT: ì§ˆë¬¸ì— ëŒ€ë‹µì„ ì˜í•˜ëŠ” ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "SFT(Supervised Fine Tuning)\n",
        "\n",
        "*   ì§ˆë¬¸ì— ì‘ë‹µì„ ì˜í•˜ë„ë¡ SFT ìˆ˜í–‰\n",
        "*   ì‚¬ëŒì´ ì§€ì‹œí•œ ëŒ€ë‹µ(13,000ê°œ), ì§ˆë¬¸ì— ì‘ë‹µ(13,000ê°œ)\n",
        "* ì§ˆë¬¸-ì‘ë‹µì˜ ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° ì…‹\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDrVciWgJ-Ug"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
        "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
        "from copy import deepcopy\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import copy\n",
        "import logging\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
        "        del state_dict\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hLeeD7wLa--",
        "outputId": "b5720d36-0b97-47d9-a4b2-cbd584d2ec4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(data_path_1_SFT='/content/drive/MyDrive/Kochatgpt_data/kochatgpt_1_SFT.jsonl', model_name='skt/kogpt2-base-v2', max_epochs=2, train_batch_size=8, output_dir='./output_1_SFT')\n"
          ]
        }
      ],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_path_1_SFT', type=str, default='/content/drive/MyDrive/Kochatgpt_data/kochatgpt_1_SFT.jsonl')\n",
        "parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--max_epochs', type=int, default=2)\n",
        "parser.add_argument('--train_batch_size', type=int, default=8)\n",
        "parser.add_argument('--output_dir', type=str, default='./output_1_SFT')\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n",
        "args.max_epochs = 2\n",
        "print(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346,
          "referenced_widgets": [
            "1a7f63b0983a404aa70e806d61ce4cde",
            "e82e8b41bf4141ff966144c3030c04fb",
            "3c4ae6a7796c4c308e3d4b6fc2d947d6",
            "376bb19053914824bea2c521e19aebfd",
            "e7126431380b42bfa5d1c8da63e8adb9",
            "c83c4f39799e4437bbaac4c5b719b3cf",
            "184f122861f54812958f73dbbfaed07c",
            "0a99ef8fdc1a4125b4fb9f820ebb4ff9",
            "3065b644811a42b5a8cf1956458f06bf",
            "9cdfaeae46604ba6b6b31f1d6910c3d1",
            "e7c5ed3baab04beb983cbc8548512dce",
            "f08a3a6d6e954e27910539f30f77c4b5",
            "f9eb369d5d0b47f28eaa46361eaabaad",
            "9b2072f474ad4cb5a8763f57e1612348",
            "e0a6ee0569b3465eabc9164ff690f242",
            "3fbefd307e8745ad80461cb32716593d",
            "df607660bfae4ea1905db15b73f9d972",
            "6ed5a71c89af4da69231ad69cc2c2c1c",
            "e9ff77598e0546a4983c00bf9dce787a",
            "e0500f612a0d41bdaced00b67930e1ef",
            "e890497e9f4d49d4bf0b81176d7445c6",
            "cac928a806ea4dfebe5ac945b750c08a",
            "04b2214e95994468a39455c5583bfaf1",
            "451c4ac026fb40a496166fadf7990657",
            "f97d1acfc2ff4219aa7951a36d9b5f54",
            "caf1e56f902a4c95a3bca6a5bc22bee6",
            "4ee49d4ee78e4a02b359e6333f95c407",
            "a330e9a5476941fcb073d6f49b1008e3",
            "86c916d9b30a4a91b928322274ce6ab4",
            "8cf7e7bddacf4b70861252a381b2f948",
            "2cc17c63fdba4033a980cee92d3de32d",
            "5de68328ea2345ff9db6d0fbaf2c209b",
            "4f9df22209354659bf30ed134ff8cf4d"
          ]
        },
        "id": "bLM29OoULp_c",
        "outputId": "7202e97c-fc2a-4e6c-e64a-1e06ba51604d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a7f63b0983a404aa70e806d61ce4cde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f08a3a6d6e954e27910539f30f77c4b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04b2214e95994468a39455c5583bfaf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?\"\n",
            "\"ê·¸ë ‡ë‹¤ë©´ ê·¸ê±´ ë°”ë¡œ 'ë‚´ê²Œ ë§ëŠ”' ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ ë°©ë²•ì€ ì•„ì£¼ ê°„ë‹¨í•©ë‹ˆë‹¤. ìš°ì„ , ë¨¼ì € ë‚´ ëª¸ì— ìˆëŠ” ì§€ë°©ì„ ì œê±°í•´ ì¤ë‹ˆë‹¤. ì§€ë°©ì„¸í¬ëŠ” ìš°ë¦¬ ëª¸ì—ì„œ ê°€ì¥ ë§ì´ ë¶„í¬ë¼ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì§€ë°©ì´ ë§ì€ ì‚¬ëŒì€ ìì‹ ì˜ ëª¸ ì†ì— ë“¤ì–´ìˆëŠ” ì§€ë°©ì˜ ì–‘ì„ ì¤„ì—¬ì•¼ í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ë‚˜ëŠ” ì´ê²ƒì„ í†µí•´ ì²´ì¤‘ì„ ê°ëŸ‰í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ ì‰½ê²Œ ì‚´ì„ ë¹¼ëŠ” ê²ƒì€ ì¢‹ì§€ ì•Šìœ¼ë‹ˆ ì£¼ì˜í•´ì•¼ê² ì£ . ì™œëƒí•˜ë©´ ì§€ë°©ì€ ëŒ€ë¶€ë¶„ ê·¼ìœ¡ê³¼ ì¸ëŒ€ ë“±ì— ë¶„í¬ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ê·¸ë§Œí¼ì˜ ì–‘ì´ ë§ê¸° ë•Œë¬¸ì´ì§€ìš”. ê·¸ëŸ¬ë¯€ë¡œ ë‹¤ì´ì–´íŠ¸ë¥¼ í•  ë•ŒëŠ” ë°˜ë“œì‹œ ì „ë¬¸ì˜ì™€ ìƒì˜í•´ì•¼ í•˜ê² ì§€ìš”?\n",
            "ì´ë ‡ê²Œ í•˜ë©´ ì‚´ì´ ì°Œê¸° ì „ì—\n"
          ]
        }
      ],
      "source": [
        "## test & load skt gpt2 kroean\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "                                                    pad_token='<pad>', mask_token='<mask>')\n",
        "print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\"))\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUPS5KClwHTs",
        "outputId": "520725ad-2f27-4d8f-e027-c1b7c78ffeaa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "                                                    pad_token='<pad>', mask_token='<mask>')\n",
        "print(len(tokenizer.tokenize(\"í•˜ëŠ˜ì€ íŒŒë€ìƒ‰ ì…ë‹ˆë‹¤.\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GSTaz1ZHMFd",
        "outputId": "f3f8e874-9189-429d-dc1d-82ced83eaf26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[{'generated_text': '0 : AIì— ê´€ì‹¬ì´ ìˆë‹ˆ?\\n: ê´œì°®ì•„.\\n: ê·¸ê±´ ê±±ì •í•˜ì§€ ë§ˆ.\\n: ì•„ë‹ˆì•¼. ì´ê²Œ ë¬´ìŠ¨ ì†Œë¦¬ì•¼? í•˜í•˜í•˜í•˜!\"\\n\"ì¢‹ì•„. ì¢‹ì•„. ê·¸ëŸ¼, ì¢€ ì‰¬ê³  ì‹¶ì—ˆì–´.\"\\n\"ê·¸ëŸ¼, ë‚˜ë‘ ê°™ì´ ê°€ì, ì‘?\"\\n\"ì‘? ì–´ë•Œ? ì•„ëƒ.'}],\n",
              " [{'generated_text': '0 : AIë¼ëŠ” ê²ƒì€ ë„ˆë¬´ ì¬ë¯¸ìˆëŠ”ê±° ê°™ì•„\\n: ë§ì•„ AIëŠ” ë„ˆë¬´ í¥ë¯¸ë¡œì›Œ \\n: ë„ˆë„ ê·¸ë ‡ê²Œ ëŠë¼ë‹ˆ? ë§ì•„ AIë¼ëŠ”ê²ƒì€ ë„ˆë¬´ ë‹¤ì–‘í•˜ê³  ì‹ ê¸°í•´\\n: ê´œì°®ì•„?\\n: ì§„ì§œ ì¬ë°Œì–´\\n'}]]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    eos_token_id=375,\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "generator(\n",
        "    [\"0 : AIì— ê´€ì‹¬ì´ ìˆë‹ˆ?\\n:\",\n",
        "    \"0 : AIë¼ëŠ” ê²ƒì€ ë„ˆë¬´ ì¬ë¯¸ìˆëŠ”ê±° ê°™ì•„\\n: ë§ì•„ AIëŠ” ë„ˆë¬´ í¥ë¯¸ë¡œì›Œ \\n: ë„ˆë„ ê·¸ë ‡ê²Œ ëŠë¼ë‹ˆ? ë§ì•„ AIë¼ëŠ”ê²ƒì€ ë„ˆë¬´ ë‹¤ì–‘í•˜ê³  ì‹ ê¸°í•´\\n:\"],\n",
        "    **generation_args\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1yApLcWLquf"
      },
      "outputs": [],
      "source": [
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\n###Input(ì…ë ¥):\\n{input}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wi0Q5PPL838",
        "outputId": "27e88528-a167-4c08-ed05-c65ab45ec012"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
          ]
        }
      ],
      "source": [
        "## ëª¨ë¸ ì¤€ë¹„\n",
        "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    args.model_name,\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=512,    \n",
        ")\n",
        "tokenizer.add_special_tokens(\n",
        "    {\n",
        "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "    }\n",
        ")    \n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHAVJPAbL_F1",
        "outputId": "e5dcd17a-429f-4e59-f4b0-cbc571cdd12b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Loading data done!!: 12000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input : tensor([14659, 13394, 37091, 10651,   383, 25841,  8006, 14914,  7673, 20479,\n",
            "         8091, 22311,  9036, 30902, 13675,   375,   424,  9792,   454,  9549,\n",
            "        20549,   383,  8142,  7192, 14914,   382, 37767, 13753,  8263,  7166,\n",
            "          739,  8352,  7659,  9594, 25585, 13600,  8022,  9378, 11532,  9887,\n",
            "        11218,  9111, 16691, 10351, 10561,  9128, 20479,  8091,  9065,  9446,\n",
            "         9036, 28420, 26521, 10163, 26367,  6958,  9030,  9882, 12317, 25882,\n",
            "         9209, 37194, 10351,  9036, 12168, 10529, 15989,  9719, 15434, 10552,\n",
            "        11188, 13362,  9036, 15805, 11300, 11846,  9146, 16691,  9181,  7397,\n",
            "        15806, 13480, 11342, 17596,  9161, 19996,  9025, 25006, 18595,  9966,\n",
            "        12592, 10751, 11814,  8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
            "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,   382, 37767, 13753,  8263,  7166,\n",
            "          739,  8352,  7659,  9594, 25585, 13600,  8022,  9378, 11532,  9887,\n",
            "        11218,  9111, 16691, 10351, 10561,  9128, 20479,  8091,  9065,  9446,\n",
            "         9036, 28420, 26521, 10163, 26367,  6958,  9030,  9882, 12317, 25882,\n",
            "         9209, 37194, 10351,  9036, 12168, 10529, 15989,  9719, 15434, 10552,\n",
            "        11188, 13362,  9036, 15805, 11300, 11846,  9146, 16691,  9181,  7397,\n",
            "        15806, 13480, 11342, 17596,  9161, 19996,  9025, 25006, 18595,  9966,\n",
            "        12592, 10751, 11814,  8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
          ]
        }
      ],
      "source": [
        "## prepare data\n",
        "from typing import Optional, Dict, Sequence\n",
        "    \n",
        "class SFT_dataset(Dataset):\n",
        "    '''SFT dataset by wygo'''\n",
        "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
        "        super(SFT_dataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        \n",
        "        ## format\n",
        "        pattern_instruction = 'prompt'  # instruction\n",
        "        pattern_input = 'input'  \n",
        "        pattern_output = 'completion'  \n",
        "\n",
        "        ############################################################\n",
        "        ## load dataset\n",
        "\n",
        "        data_path_1_SFT = '/content/drive/MyDrive/Kochatgpt_data/kochatgpt_1_SFT.jsonl'\n",
        "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
        "            list_data_dict = json.load(json_file)\n",
        "            if verbose:\n",
        "                print('## data check ##')\n",
        "                print((list_data_dict[0]))\n",
        "\n",
        "        ############################################################\n",
        "        ## ë°ì´í„°ì…‹ ë§Œë“¤ê¸°, sourceì™€ target\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°\n",
        "\n",
        "        # ì…ë ¥\n",
        "        sources = []\n",
        "        for example in list_data_dict:\n",
        "            if example.get(pattern_input, \"\") != \"\":\n",
        "                tmp = prompt_input.format_map(example)\n",
        "            else:\n",
        "                tmp = prompt_no_input.format_map(example)\n",
        "            sources.append(tmp)\n",
        "\n",
        "        # ì¶œë ¥\n",
        "        targets = []\n",
        "        for example in list_data_dict:\n",
        "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
        "\n",
        "        if verbose:\n",
        "            idx = 0\n",
        "            print((sources[idx]))\n",
        "            print((targets[idx]))\n",
        "            print(\"Tokenizing inputs... This may take some time...\")\n",
        "\n",
        "        ############################################################\n",
        "        examples = [s + t for s, t in zip(sources, targets)]\n",
        "\n",
        "        # source data tokenized\n",
        "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # sourceë§Œ\n",
        "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
        "\n",
        "\n",
        "        ## ì…ë ¥ì€ source, ì¶œë ¥ì€ source+target ì´ì§€ë§Œ í•™ìŠµì€ target ë¶€ë¶„ë§Œ\n",
        "        input_ids = examples_tokenized[\"input_ids\"]\n",
        "        labels = copy.deepcopy(input_ids)\n",
        "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "            label[:source_len] = IGNORE_INDEX  # source ë¶€ë¶„ì€ -100ìœ¼ë¡œ ì±„ìš´ë‹¤\n",
        "        \n",
        "        data_dict = dict(input_ids=input_ids, labels=labels)        \n",
        "        \n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))    \n",
        "        \n",
        "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "        \"\"\"Tokenize a list of strings.\"\"\"\n",
        "        tokenized_list = [\n",
        "            tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"longest\",\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "            )\n",
        "            for text in strings\n",
        "        ]\n",
        "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "        input_ids_lens = labels_lens = [\n",
        "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "        ]\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            input_ids_lens=input_ids_lens,\n",
        "            labels_lens=labels_lens,\n",
        "        )        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n",
        "eval_dataset  = None  # evalì€ ì•ˆí•¨\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "# check\n",
        "print('input : %s'%train_dataset.input_ids[0])\n",
        "print('output: %s'%train_dataset.labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3wy8oGVgi_4",
        "outputId": "bcf09288-f136-4a0c-998f-3a5ba4a28aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "id": "Vi1iZ5VXMo9v",
        "outputId": "2b63e672-fa29-4ebb-acbc-47d1af1ec1ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='355' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 355/3000 59:35 < 7:26:28, 0.10 it/s, Epoch 0.12/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## í•™ìŠµ (10min)\n",
        "# training_args ìˆ˜ì • ê°€ëŠ¥: https://github.com/Beomi/KoAlpaca/blob/main/train.sh ì°¸ê³ \n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…¡á†¼á„‹á…´/GPT\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=1, # number of training epochs\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    eval_steps = 3, # Number of update steps between two evaluations.\n",
        "    save_steps=500, # after # steps model is saved \n",
        "    warmup_steps=5,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    )\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPct4AxNM_gA"
      },
      "source": [
        "##GPT2ëª¨ë¸ì´ ì‚¬ëŒì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëŒ€ë‹µí•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_Ni7ztKM2Id"
      },
      "outputs": [],
      "source": [
        "## ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
        "generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)\n",
        "# generator = pipeline('text-generation', model=model.cpu(), tokenizer=tokenizer, config={'max_length':800})\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0, #ë°˜ë³µë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì–µì œí•¨\n",
        "    no_repeat_ngram_size=4, #ê°œì˜ ì—°ì†ì ì¸ ë‹¨ì–´ë¥¼ ë°˜ë³µí•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
        "    eos_token_id=375, #EOS(End-of-Sequence)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í† í° IDë¥¼ ì§€ì • ë° í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ë•Œ ëª¨ë¸ì€ ì´ EOS í† í°ì— ë„ë‹¬í•˜ë©´ í† í° ìƒì„±ì„ ì¤‘ì§€\n",
        "    max_new_tokens=64,#ìƒì„±í•˜ëŠ” í† í°ì˜ ìµœëŒ€ ìˆ˜\n",
        "    do_sample=True, #Trueë¡œ ì„¤ì •í•˜ë©´ ëª¨ë¸ì´ ì˜ˆì¸¡ í™•ë¥ ì— ë”°ë¼ ë‹¤ìŒ í† í°ì„ ë¬´ì‘ìœ„ ì„ íƒ \n",
        "    top_k=50, #ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í• ë•Œ ì˜ˆì¸¡ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ìƒìœ„ k ê°œë¡œ ì˜ˆì¸¡í•¨\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "list_prompt = [\"AIë€ ë¬´ì—‡ì¸ê°€?\", \"ê°•ì›ë„ëŠ” ë¬´ì—‡ì´ ìœ ëª…í•œê°€ìš”?\"]\n",
        "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
        "\n",
        "list_result = generator(list_prompt, **generation_args)\n",
        "for prompt, result in zip(list_prompt, list_result):\n",
        "    print((\"-----------------Chat AIë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.-----------------\\n\"))\n",
        "    print(('ì§ˆë¬¸ : {}\\n\\n').format(result[0]['generated_text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLGRyTPpNMQX"
      },
      "source": [
        "##Step 2) RM : ì¢‹ì€ ê¸€ ì±„ì ê¸° ë§Œë“¤ê¸°\n",
        "\n",
        "* ìƒì„±ëœ ê° ê¸€ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  ì´ë¥¼ ë³´ìƒëª¨ë¸ë¡œ ë§Œë“¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9-DqeHDg42G"
      },
      "outputs": [],
      "source": [
        "## setup(1min)\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "!pip install torch==1.13.1\n",
        "# setup data\n",
        "\n",
        "%cd /content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…¡á†¼á„‹á…´/GPT/colossalai_ChatGPT/\n",
        "!pip install .\n",
        "%cd ../../\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4yQwLBnBFof"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mZJmQZiNQ2G"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "import loralib as lora\n",
        "import torch\n",
        "from chatgpt.dataset import RewardDataset\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.models.bloom import BLOOMRM\n",
        "from chatgpt.models.gpt import GPTRM\n",
        "from chatgpt.models.opt import OPTRM\n",
        "from chatgpt.trainer import RewardModelTrainer\n",
        "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
        "from datasets import load_dataset\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from colossalai.nn.optimizer import HybridAdam\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n Response(ì‘ë‹µ):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\n Response(ì‘ë‹µ):\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlYLhC-7NmuW"
      },
      "outputs": [],
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--output_dir', type=str, default='./output_2_RM')\n",
        "parser.add_argument('--data_path_2_RM', type=str, default='/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…¡á†¼á„‹á…´/GPT/data_kochatgpt/kochatgpt_2_RM_custom.jsonl')\n",
        "parser.add_argument('--strategy',\n",
        "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
        "                    default='naive')\n",
        "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n",
        "parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n",
        "parser.add_argument('--max_epochs', type=int, default=10)\n",
        "parser.add_argument('--batch_size', type=int, default=4)\n",
        "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument('--max_len', type=int, default=512)  # wygo ì¶”ê°€\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.max_epochs = 3\n",
        "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
        "args.verbose = True\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C7fHvYGOGg9"
      },
      "outputs": [],
      "source": [
        "# configure strategy\n",
        "if args.strategy == 'naive':\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == 'ddp':\n",
        "    strategy = DDPStrategy()\n",
        "elif args.strategy == 'colossalai_gemini':\n",
        "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
        "elif args.strategy == 'colossalai_zero2':\n",
        "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgSF1K2uOLNU"
      },
      "outputs": [],
      "source": [
        "# customizing, https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/nn/gpt_rm.py#L29\n",
        "from typing import Optional\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "\n",
        "# from ..base import RewardModel\n",
        "from chatgpt.models.base import RewardModel\n",
        "\n",
        "\n",
        "class GPTRM_custom(RewardModel):\n",
        "    \"\"\"\n",
        "    GPT Reward model.\n",
        "    Args:\n",
        "        pretrained (str): Pretrained model name or path.\n",
        "        config (GPT2Config): Model config.\n",
        "        checkpoint (bool): Enable gradient checkpointing.\n",
        "        lora_rank (int): Rank of the low-rank approximation.\n",
        "        lora_train_bias (str): LoRA bias training mode.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 pretrained: Optional[str] = None,\n",
        "                 config: Optional[GPT2Config] = None,\n",
        "                 checkpoint: bool = False,\n",
        "                 lora_rank: int = 0,\n",
        "                 lora_train_bias: str = 'none',\n",
        "                 tokenizer=None) -> None:\n",
        "        if pretrained is not None:\n",
        "            model = GPT2Model.from_pretrained(pretrained)\n",
        "            model.resize_token_embeddings(len(tokenizer))  # wygo ì¶”ê°€!!!\n",
        "        elif config is not None:\n",
        "            model = GPT2Model(config)\n",
        "        else:\n",
        "            model = GPT2Model(GPT2Config())\n",
        "        if checkpoint:\n",
        "            model.gradient_checkpointing_enable()\n",
        "\n",
        "        # model = model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        value_head = nn.Linear(model.config.n_embd, 1)\n",
        "        super().__init__(model, value_head, lora_rank, lora_train_bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab2NqplAOPOU"
      },
      "outputs": [],
      "source": [
        "# configure model, tokenizer\n",
        "with strategy.model_init_context():\n",
        "    # load pretrained gpt2    \n",
        "    if args.model == 'gpt2':\n",
        "#         tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        # tokenizer = AutoTokenizer.from_pretrained(args.pretrain)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model = GPTRM_custom(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n",
        "\n",
        "    elif args.model == 'bloom':\n",
        "        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
        "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
        "    \n",
        "    elif args.model == 'opt':\n",
        "        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")      \n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
        "    \n",
        "    \n",
        "    # model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr2hPwPFOS6V"
      },
      "outputs": [],
      "source": [
        "# make ranking data to chosen, rejetced data\n",
        "with open(args.data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    if args.verbose:\n",
        "        print('## data check ##')\n",
        "        print((list_data_dict[0]))\n",
        "        \n",
        "total_data_ranking2chosen = []\n",
        "for tmp in list_data_dict:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    # data 1) 0 VS 1\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "\n",
        "    # data 2) 0 VS 2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # data 1) 1 VS 2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "    \n",
        "    \n",
        "    \n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "print('before data num: %d'%(len(list_data_dict)))\n",
        "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
        "print('data example: \\n%s'%total_data_ranking2chosen[45])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85ca93LKB5xN"
      },
      "outputs": [],
      "source": [
        "len(total_data_ranking2chosen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv358luNOX8W"
      },
      "outputs": [],
      "source": [
        "# prepare for data and dataset\n",
        "import random\n",
        "random.seed(230319)\n",
        "# list_tmp = list(range(10))\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "print(total_data_ranking2chosen[45])\n",
        "\n",
        "# train_data = total_data_ranking2chosen[:-1000]  # 29000 í•™ìŠµ\n",
        "# eval_data = total_data_ranking2chosen[-1000:0]  # 1000ê°œë§Œ í‰ê°€\n",
        "\n",
        "train_data = total_data_ranking2chosen[:100]  # í•™ìŠµ ë°ì´í„°ì˜ ìˆ˜\n",
        "eval_data = total_data_ranking2chosen[100:130]  # í‰ê°€ ë°ì´í„°ì˜ ìˆ˜\n",
        "\n",
        "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
        "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
        "\n",
        "# check\n",
        "idx = 10\n",
        "print('#'*70)\n",
        "print('## prompt ##')\n",
        "print(train_data[idx]['prompt'])\n",
        "print('#'*70)\n",
        "print('## chosen ##')\n",
        "print(train_data[idx]['chosen'])\n",
        "print('#'*70)\n",
        "print('## rejected ##')\n",
        "print(train_data[idx]['rejected'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq99mcy2OjIT"
      },
      "outputs": [],
      "source": [
        "# configure optimizer\n",
        "if args.strategy.startswith('colossalai'):\n",
        "    optim = HybridAdam(model.parameters(), lr=5e-5)\n",
        "else:\n",
        "    optim = Adam(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ua2vKRgOkjr"
      },
      "outputs": [],
      "source": [
        "# batch_size here is expected to be C(k,2), k means # response of each prompt\n",
        "# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n",
        "trainer = RewardModelTrainer(model=model,\n",
        "                             strategy=strategy,\n",
        "                             optim=optim,\n",
        "                             train_dataset=train_dataset,\n",
        "                             eval_dataset=eval_dataset,\n",
        "                             batch_size=args.batch_size,\n",
        "                             max_epochs=args.max_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m25-9pd7Oq3d"
      },
      "source": [
        "##RMì„ í†µí•´ ê° ìˆœìœ„ì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  ê°€ì¥ ì •í™•í•œ ì‘ë‹µì— ëŒ€í•´ í•™ìŠµí•¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5DeYx6jOmWv"
      },
      "outputs": [],
      "source": [
        "# train!!\n",
        "trainer.fit(use_lora=args.lora_rank)\n",
        "\n",
        "## save\n",
        "# save model checkpoint after fitting on only rank0\n",
        "strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n",
        "# save optimizer checkpoint on all ranks\n",
        "strategy.save_optimizer(optim,\n",
        "                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
        "                        only_rank0=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vFmdP46P3cz"
      },
      "source": [
        "## Step 3) PPO ì‚¬ëŒì˜ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì´ í”¼ë“œë°±ê³¼ ë¹„ìŠ·í•´ ì§€ë„ë¡ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_vRtPXodlmf"
      },
      "outputs": [],
      "source": [
        "## setup(1min)\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "!pip install torch==1.13.1\n",
        "# setup data\n",
        "\n",
        "%cd /content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…¡á†¼á„‹á…´/GPT/colossalai_ChatGPT/\n",
        "!pip install .\n",
        "%cd ../../\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTa3nVDuO5Nk"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n",
        "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
        "from chatgpt.models.opt import OPTActor, OPTCritic\n",
        "from chatgpt.trainer import PPOTrainer\n",
        "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from colossalai.nn.optimizer import HybridAdam\n",
        "\n",
        "## wy ì¶”ê°€\n",
        "import json\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "## clossalAI error í•´ê²°\n",
        "os.environ['RANK'] = '0'\n",
        "os.environ['LOCAL_RANK'] = '0'\n",
        "os.environ['WORLD_SIZE'] = '2'\n",
        "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "os.environ['MASTER_PORT'] = '42043'\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzLnJEHZPB2T"
      },
      "outputs": [],
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_path_3_PPO', type=str, default='/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…¡á†¼á„‹á…´/GPT/data_kochatgpt/kochatgpt_3_PPO.jsonl')\n",
        "parser.add_argument('--output_dir', type=str, default='./output_3_PPO')\n",
        "parser.add_argument('--strategy',\n",
        "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
        "                    default='naive')\n",
        "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--num_episodes', type=int, default=10)\n",
        "parser.add_argument('--max_timesteps', type=int, default=3)\n",
        "parser.add_argument('--update_timesteps', type=int, default=3)\n",
        "parser.add_argument('--max_epochs', type=int, default=5)\n",
        "parser.add_argument('--train_batch_size', type=int, default=8)\n",
        "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument('--max_length', type=int, default=250)\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.output_dir = './output_3_PPO'\n",
        "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
        "\n",
        "# args.pretrain_actor = './output_1_SFT'  # SFT ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
        "# args.pretrain_critic = './output_2_RM'  # RM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
        "args.pretrain_actor = args.pretrain\n",
        "args.pretrain_critic = args.pretrain\n",
        "\n",
        "args.num_episodes = 1\n",
        "args.max_epochs   = 1\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pisr96oIccRT"
      },
      "outputs": [],
      "source": [
        "args.pretrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9xRGXwiPNs3"
      },
      "outputs": [],
      "source": [
        "# configure strategy\n",
        "if args.strategy == 'naive':\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == 'ddp':\n",
        "    strategy = DDPStrategy()\n",
        "elif args.strategy == 'colossalai_gemini':\n",
        "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
        "elif args.strategy == 'colossalai_zero2':\n",
        "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og7ueBVvPPtH"
      },
      "outputs": [],
      "source": [
        "# configure model, tokenizer\n",
        "with strategy.model_init_context():\n",
        "    if args.model == 'gpt2':\n",
        "        actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        # tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )    \n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "\n",
        "    elif args.model == 'bloom':\n",
        "        actor = BLOOMActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        critic = BLOOMCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
        "        tokenizer.pad_token = tokenizer.eos_token            \n",
        "    elif args.model == 'opt':\n",
        "        actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        critic = OPTCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")            \n",
        "    else:\n",
        "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
        "\n",
        "    initial_model = deepcopy(actor)\n",
        "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DhMVvYjPR2n"
      },
      "outputs": [],
      "source": [
        "# configure optimizer\n",
        "if args.strategy.startswith('colossalai'):\n",
        "    actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n",
        "    critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n",
        "else:\n",
        "    actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
        "    critic_optim = Adam(critic.parameters(), lr=5e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp_dgLebPTe3"
      },
      "outputs": [],
      "source": [
        "# setting the models\n",
        "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n",
        "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThJWdu2aPVD-"
      },
      "outputs": [],
      "source": [
        "# prepare data\n",
        "with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
        "\n",
        "def tokenize_fn(texts):\n",
        "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
        "    return {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "print(list_prompt)\n",
        "print('\\n\\n\\n')\n",
        "print(tokenize_fn('I want you to act as a linux terminal.'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mctryMQGPV6y"
      },
      "outputs": [],
      "source": [
        "# configure trainer\n",
        "trainer = PPOTrainer(strategy,\n",
        "                     actor,\n",
        "                     critic,\n",
        "                     reward_model,\n",
        "                     initial_model,\n",
        "                     actor_optim,\n",
        "                     critic_optim,\n",
        "                     max_epochs=args.max_epochs,\n",
        "                     train_batch_size=args.train_batch_size,\n",
        "                     tokenizer=tokenize_fn,\n",
        "                     max_length=128,\n",
        "                     do_sample=True,\n",
        "                     temperature=1.0,\n",
        "                     top_k=50,\n",
        "                     pad_token_id=tokenizer.pad_token_id,\n",
        "                     eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "## train!\n",
        "trainer.fit(list_prompt,  # ì…ë ¥ prompt\n",
        "            num_episodes=args.num_episodes,\n",
        "            max_timesteps=args.max_timesteps,\n",
        "            update_timesteps=args.update_timesteps)\n",
        "\n",
        "## save\n",
        "# save model checkpoint after fitting on only rank0\n",
        "strategy.save_model(actor, os.path.join(args.output_dir, 'actor.pt'), only_rank0=True)\n",
        "# save optimizer checkpoint on all ranks\n",
        "strategy.save_optimizer(actor_optim,\n",
        "                        os.path.join(args.output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
        "                        only_rank0=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ydF3RbQE0Uy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXGgCwz5KH01"
      },
      "outputs": [],
      "source": [
        "actor=GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ahSVzSxPZdS"
      },
      "outputs": [],
      "source": [
        "## inference\n",
        "def generation(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device())\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=args.max_length,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print(\"-\"*70)\n",
        "    print((\"ì±—ë´‡ : {}\").format(output))\n",
        "    print(\"-\"*70)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ldehpLuJEk_"
      },
      "outputs": [],
      "source": [
        "list_prompt = [\"ì¸ê³µì§€ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ ì„¤ëª…í•´ì¤˜\",\"í•˜ëŠ˜ì—ëŠ” ì™œ êµ¬ë¦„ì´ ìˆì„ê¹Œ?\"]\n",
        "\n",
        "for input_text in list_prompt:\n",
        "  print(\"-\"*70)\n",
        "  print((\"ì§ˆë¬¸ : {}\").format(input_text))\n",
        "  output = generation(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QA-sQ0ig5M2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h16qOhQug5J_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdUS_YHlhXeu"
      },
      "source": [
        "##í•™ìŠµ ë°ì´í„° í™œìš©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgXMqdh5g5Hu"
      },
      "outputs": [],
      "source": [
        "# import\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "from chatgpt.models.bloom import BLOOMActor\n",
        "from chatgpt.models.gpt import GPTActor\n",
        "from chatgpt.models.opt import OPTActor\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynju55ZIg7_t",
        "outputId": "cf77da63-1b18-4b82-a1c9-16a625a8b64d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model',\n",
        "                    default='gpt2',\n",
        "                    choices=['gpt2', 'bloom', 'opt'])\n",
        "# We suggest to use the pretrained model from HuggingFace, use pretrain to configure model\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--model_path', type=str, default=None)\n",
        "parser.add_argument('--input',\n",
        "                    type=str,\n",
        "                    default='Question: How are you ? Answer:')\n",
        "parser.add_argument('--max_length', type=int, default=250)\n",
        "args_inference = parser.parse_args([])\n",
        "\n",
        "args_inference.model = 'gpt2'\n",
        "args_inference.pretrain = 'skt/kogpt2-base-v2'\n",
        "args_inference.model_directory = '/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼á„€á…¡á†¼á„‹á…´/output_3_PPO'\n",
        "args_inference.model_path = os.path.join(args_inference.model_directory, 'actor.pt')\n",
        "\n",
        "# configure model, tokenizer\n",
        "if args_inference.model == 'gpt2':\n",
        "    #actor = GPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
        "    actor = GPTActor(pretrained=args_inference.pretrain)\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained(args_inference.pretrain)\n",
        "    # tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args_inference.pretrain,\n",
        "                                              padding_side=\"right\",\n",
        "                                              model_max_length=512)\n",
        "    tokenizer.add_special_tokens({\n",
        "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "    })\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "elif args_inference.model == 'bloom':\n",
        "    actor = BLOOMActor(pretrained=args_inference.pretrain).to(\n",
        "        torch.cuda.current_device())\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args_inference.model == 'opt':\n",
        "    actor = OPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
        "    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported model \"{args_inference.model}\"')\n",
        "\n",
        "state_dict = torch.load(args_inference.model_path, map_location='cpu');\n",
        "actor.model.load_state_dict(state_dict);\n",
        "\n",
        "actor.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XGTiOOJhLaG"
      },
      "outputs": [],
      "source": [
        "## inference\n",
        "def generation(input_text):\n",
        "    #input_ids = tokenizer.encode(input_text, return_tensors='pt').to(torch.cuda.current_device())\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=args_inference.max_length,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print(\"-\"*70)\n",
        "    print((\"ì±—ë´‡ : {}\").format(output))\n",
        "    print(\"-\"*70)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P4Qp1hqhNPW",
        "outputId": "2f0fbabf-82b3-4db2-d6fb-bf49f10bfebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------\n",
            "ì§ˆë¬¸ : ì¸ê³µì§€ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ ì„¤ëª…í•´ì¤˜\n",
            "----------------------------------------------------------------------\n",
            "ì±—ë´‡ : ì¸ê³µì§€ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ ì„¤ëª…í•´ì¤˜ìš”.\n",
            "ì–´ë¨¸ë‹˜ì´ ë³´ì‹œëŠ” ê²Œ ë°”ë¡œ ê·¸ê±°ì˜ˆìš”?\n",
            "ì–´ë¨¸ë‹˜ì€ ê·¸ê²Œ ë­”ì§€ ì•„ì„¸ìš”?\n",
            "ì•„ë‹ˆ ê·¸ëƒ¥ ê·¸~ ì•Œ ìˆ˜ê°€ ì—†ëŠ” ê±´ë° ì™œ ê·¸~ ê·¸~ ê·¸~ ë´‡ë¬¼ í„°ì§€ëŠ” ê·¸~ ì˜ìƒì„ ë³´ê³  ì§„ì§œ ì–´ë¨¸ë‹˜ì€ ì–¼ë§ˆë‚˜ ì´ì˜ê²Œ ë³´ì…¨ì„ê¹Œë¼ê³  ìƒê°í•˜ì‹œê² ì§€ë§Œ ê·¸~ ê·¸~ ê·¸~ ê·¸~ ì–´ë¨¸ë‹˜ì´ ì´ê²Œ ë¬´ìŠ¨ ì†Œë¦´ í•˜ì‹œëŠ”ì§€ ëª°ëì§€ë§Œ ì´ì œ ê·¸~ ë­ë„ê¹Œ ë˜ê²Œ ê¶ê¸ˆí•˜ì–ì•„ìš”.\n",
            "ê·¸~ ê·¸~ ê·¸~ ì•„~ ê·¸~ ê·¸~ ê·¸~ ê·¸~ ë‹ˆê¹Œ?\n",
            "ì•„~ ê·¸ë˜ì„œ ê·¸~ ì•„~ ì œê°€ ê·¸~ ì§€ê¸ˆ ê·¸~ ë‹ˆê¹Œ?\n",
            "ê·¸ë˜ì„œ ê·¸~ ê·¸~ ë‹ˆê¹Œ?\n",
            "ê·¸~ ì•„~ ì•„ë‹ˆ ê·¼ë° ì•„ë‹ˆ ê·¸~ ë‹ˆê¹Œ?\n",
            "ê·¸~ ê·¸~ ì•„~ ê·¸ëŸ¼ ì–´~ ë­ì•¼ ì•„~ ê·¸~ ì•„~ ì–´~ ì´~ ê·¸~ ë­”ì§€ ê¶ê¸ˆí•˜ì–ì•„ìš”.\n",
            "ê·¸~ ë‹ˆ?\n",
            "ê·¸~ ê·¸~ ë‹ˆê¹Œ?\n",
            "ê·¸~ ì•„~ ë­”ì§€ ê¶ê¸ˆí•œ ê±°ì˜ˆìš”?\n",
            "ì•„ë‹ˆ ë‹ˆê¹Œ?\n",
            "ë‹ˆê¹Œ?\n",
            "ì•„ë‹ˆ ë­ì•¼ ê·¸ë˜ì„œ ë‹ˆê¹Œ?\n",
            "ì•„ë‹ˆ ê·¸~ ì €í¬ ì•„~ ê·¸~\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "ì§ˆë¬¸ : í•˜ëŠ˜ì—ëŠ” ì™œ êµ¬ë¦„ì´ ìˆì„ê¹Œ?\n",
            "----------------------------------------------------------------------\n",
            "ì±—ë´‡ : í•˜ëŠ˜ì—ëŠ” ì™œ êµ¬ë¦„ì´ ìˆì„ê¹Œ? ì–´ë•Œ? ì§€ê¸ˆì´ ë•¡ë•¡ì´ì´ì•¼. ìš°ë¦¬ ì§‘ì´ ë‹¤ ì¢‹ì•˜ì§€.\"\n",
            "ê·¸ ë§ì— ë†€ë€ ì—„ë§ˆëŠ” ë§í–ˆë‹¤.\n",
            "\"ë‚˜ëŠ” ë‹¤ë“¤ ì¢‹ì•˜ì–´. ë‚œ ì—„ë§ˆí•œí…Œ ì˜ëì–´. ë„ˆí¬ë“¤ ë‹¤ ì˜ëì–´. ë‚´ ì•„ë“¤ë„ ì—„ë§ˆê°€ ì˜ëìœ¼ë©´ ì¢‹ê² ë‹¤ê³ . ìš°ë¦¬ ì§‘ ì‚¬ëŒë“¤ì´ ì˜ë¼ì„œ ì¢‹ì•˜ì–´.\"\n",
            "\"ê·¸ë ‡ë‹¤ë©´ ì´ì œ ì§‘ìœ¼ë¡œ ëŒì•„ê°€ë„ ë¼?\"\n",
            "\"ë‚´ê°€ ì—„ë§ˆí•œí…Œ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ê·¸ëŸ¼, ìš°ë¦¬ ì§‘ ì‚¬ëŒë“¤ì—ê²Œ ì˜ë  ê±°ì•¼. ìš°ë¦¬ ì§‘ ì‚¬ëŒë“¤ì€ ë‹¤ ì˜ëì–´. ë‚´ ì•„ë“¤ë„ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ì´ë ‡ê²Œ ë§í–ˆê±°ë“ . ë‚´ê°€ ì—„ë§ˆí•œí…Œ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´. ë‚´ ì•„ë“¤ë„ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ì—„ë§ˆ. ìš°ë¦¬ ì§‘ ì‚¬ëŒë“¤ ë‹¤ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´. ì—„ë§ˆëŠ” ë•¡ë•¡ì´ë„ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ê·¸ë˜. ìš°ë¦¬ ì§‘ ì‚¬ëŒë“¤ì—ê²Œ ì˜ëì–´. ìš°ë¦¬ ë™ë„¤ ì‚¬ëŒë“¤ì€ ë‹¤ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ê·¸ëŸ¼, ì—„ë§ˆ. ì´ê²Œ ì—„ë§ˆê°€ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´. ì—„ë§ˆê°€ ë•¡ë•¡ì´ë¥¼ ë§ì´ ì˜í–ˆìœ¼ë©´ ì¢‹ê² ì–´. ì—„ë§ˆëŠ” ë‹¤ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ê·¸ëŸ¼ ë‚´ê°€ ë„ˆ\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "list_prompt = [\"ì¸ê³µì§€ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ ì„¤ëª…í•´ì¤˜\",\"í•˜ëŠ˜ì—ëŠ” ì™œ êµ¬ë¦„ì´ ìˆì„ê¹Œ?\"]\n",
        "\n",
        "for input_text in list_prompt:\n",
        "  print(\"-\"*70)\n",
        "  print((\"ì§ˆë¬¸ : {}\").format(input_text))\n",
        "  output = generation(input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKFjohc4OGLk"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04b2214e95994468a39455c5583bfaf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_451c4ac026fb40a496166fadf7990657",
              "IPY_MODEL_f97d1acfc2ff4219aa7951a36d9b5f54",
              "IPY_MODEL_caf1e56f902a4c95a3bca6a5bc22bee6"
            ],
            "layout": "IPY_MODEL_4ee49d4ee78e4a02b359e6333f95c407"
          }
        },
        "0a99ef8fdc1a4125b4fb9f820ebb4ff9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "184f122861f54812958f73dbbfaed07c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a7f63b0983a404aa70e806d61ce4cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e82e8b41bf4141ff966144c3030c04fb",
              "IPY_MODEL_3c4ae6a7796c4c308e3d4b6fc2d947d6",
              "IPY_MODEL_376bb19053914824bea2c521e19aebfd"
            ],
            "layout": "IPY_MODEL_e7126431380b42bfa5d1c8da63e8adb9"
          }
        },
        "2cc17c63fdba4033a980cee92d3de32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3065b644811a42b5a8cf1956458f06bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "376bb19053914824bea2c521e19aebfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cdfaeae46604ba6b6b31f1d6910c3d1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e7c5ed3baab04beb983cbc8548512dce",
            "value": " 2.83M/2.83M [00:00&lt;00:00, 10.6MB/s]"
          }
        },
        "3c4ae6a7796c4c308e3d4b6fc2d947d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a99ef8fdc1a4125b4fb9f820ebb4ff9",
            "max": 2825034,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3065b644811a42b5a8cf1956458f06bf",
            "value": 2825034
          }
        },
        "3fbefd307e8745ad80461cb32716593d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "451c4ac026fb40a496166fadf7990657": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a330e9a5476941fcb073d6f49b1008e3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_86c916d9b30a4a91b928322274ce6ab4",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "4ee49d4ee78e4a02b359e6333f95c407": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f9df22209354659bf30ed134ff8cf4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5de68328ea2345ff9db6d0fbaf2c209b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ed5a71c89af4da69231ad69cc2c2c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86c916d9b30a4a91b928322274ce6ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cf7e7bddacf4b70861252a381b2f948": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b2072f474ad4cb5a8763f57e1612348": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9ff77598e0546a4983c00bf9dce787a",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0500f612a0d41bdaced00b67930e1ef",
            "value": 1000
          }
        },
        "9cdfaeae46604ba6b6b31f1d6910c3d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a330e9a5476941fcb073d6f49b1008e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c83c4f39799e4437bbaac4c5b719b3cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cac928a806ea4dfebe5ac945b750c08a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caf1e56f902a4c95a3bca6a5bc22bee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5de68328ea2345ff9db6d0fbaf2c209b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4f9df22209354659bf30ed134ff8cf4d",
            "value": " 513M/513M [00:06&lt;00:00, 69.2MB/s]"
          }
        },
        "df607660bfae4ea1905db15b73f9d972": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0500f612a0d41bdaced00b67930e1ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0a6ee0569b3465eabc9164ff690f242": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e890497e9f4d49d4bf0b81176d7445c6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cac928a806ea4dfebe5ac945b750c08a",
            "value": " 1.00k/1.00k [00:00&lt;00:00, 9.09kB/s]"
          }
        },
        "e7126431380b42bfa5d1c8da63e8adb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7c5ed3baab04beb983cbc8548512dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e82e8b41bf4141ff966144c3030c04fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c83c4f39799e4437bbaac4c5b719b3cf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_184f122861f54812958f73dbbfaed07c",
            "value": "Downloading (â€¦)/main/tokenizer.json: 100%"
          }
        },
        "e890497e9f4d49d4bf0b81176d7445c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9ff77598e0546a4983c00bf9dce787a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f08a3a6d6e954e27910539f30f77c4b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9eb369d5d0b47f28eaa46361eaabaad",
              "IPY_MODEL_9b2072f474ad4cb5a8763f57e1612348",
              "IPY_MODEL_e0a6ee0569b3465eabc9164ff690f242"
            ],
            "layout": "IPY_MODEL_3fbefd307e8745ad80461cb32716593d"
          }
        },
        "f97d1acfc2ff4219aa7951a36d9b5f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cf7e7bddacf4b70861252a381b2f948",
            "max": 513302779,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cc17c63fdba4033a980cee92d3de32d",
            "value": 513302779
          }
        },
        "f9eb369d5d0b47f28eaa46361eaabaad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df607660bfae4ea1905db15b73f9d972",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6ed5a71c89af4da69231ad69cc2c2c1c",
            "value": "Downloading (â€¦)lve/main/config.json: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}