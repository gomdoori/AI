{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gomdoori/user1/blob/main/Chatgpt%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsxhb4RzJ7MQ"
   },
   "source": [
    "##step0) Colab 환경 설정\n",
    "설치(python>=3.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zj7Xp2LBJ3cF",
    "outputId": "5bc32e4e-d33f-4278-e3d7-f21335721135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting colossalai==0.2.7\n",
      "  Downloading colossalai-0.2.7.tar.gz (686 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.7/686.7 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.22.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.65.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (23.1)\n",
      "Collecting pre-commit (from colossalai==0.2.7)\n",
      "  Downloading pre_commit-3.3.2-py2.py3-none-any.whl (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.3.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.3)\n",
      "Collecting fabric (from colossalai==0.2.7)\n",
      "  Downloading fabric-3.0.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contexttimer (from colossalai==0.2.7)\n",
      "  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting ninja (from colossalai==0.2.7)\n",
      "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (2.0.1+cu118)\n",
      "Collecting invoke>=2.0 (from fabric->colossalai==0.2.7)\n",
      "  Downloading invoke-2.1.2-py3-none-any.whl (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting paramiko>=2.4 (from fabric->colossalai==0.2.7)\n",
      "  Downloading paramiko-3.1.0-py3-none-any.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cfgv>=2.0.0 (from pre-commit->colossalai==0.2.7)\n",
      "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting identify>=1.0.0 (from pre-commit->colossalai==0.2.7)\n",
      "  Downloading identify-2.5.24-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit->colossalai==0.2.7)\n",
      "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0)\n",
      "Collecting virtualenv>=20.10.0 (from pre-commit->colossalai==0.2.7)\n",
      "  Downloading virtualenv-20.23.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->colossalai==0.2.7) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->colossalai==0.2.7) (16.0.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (67.7.2)\n",
      "Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric->colossalai==0.2.7)\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (40.0.2)\n",
      "Collecting pynacl>=1.5 (from paramiko>=2.4->fabric->colossalai==0.2.7)\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting distlib<1,>=0.3.6 (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7)\n",
      "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->colossalai==0.2.7) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->colossalai==0.2.7) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
      "Building wheels for collected packages: colossalai, contexttimer\n",
      "  Building wheel for colossalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for colossalai: filename=colossalai-0.2.7-py3-none-any.whl size=896479 sha256=a31b61c79c201a040ae797a926740e8d13199d2fb03c0cdb70bea2b18476734c\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/85/25/32a3af943ea5ca261b1b51dae74a4629599ce1bc6fe58dbbfc\n",
      "  Building wheel for contexttimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5803 sha256=f7f022ba80cdf4f2bce8749ec07529c9de7f9b3ec64b548af5b79c8daaea2649\n",
      "  Stored in directory: /root/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4\n",
      "Successfully built colossalai contexttimer\n",
      "Installing collected packages: ninja, distlib, contexttimer, virtualenv, nodeenv, invoke, identify, cfgv, bcrypt, pynacl, pre-commit, paramiko, fabric, colossalai\n",
      "Successfully installed bcrypt-4.0.1 cfgv-3.3.1 colossalai-0.2.7 contexttimer-0.3.3 distlib-0.3.6 fabric-3.0.1 identify-2.5.24 invoke-2.1.2 ninja-1.11.1 nodeenv-1.8.0 paramiko-3.1.0 pre-commit-3.3.2 pynacl-1.5.0 virtualenv-20.23.0\n",
      "/content/drive/MyDrive/인공지능강의/GPT/colossalai_ChatGPT\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Processing /content/drive/MyDrive/인공지능강의/GPT/colossalai_ChatGPT\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting transformers>=4.20.1 (from chatgpt==0.1.0)\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.65.0)\n",
      "Collecting datasets (from chatgpt==0.1.0)\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting loralib (from chatgpt==0.1.0)\n",
      "  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: colossalai>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.2.7)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.0.1+cu118)\n",
      "Collecting langchain (from chatgpt==0.1.0)\n",
      "  Downloading langchain-0.0.177-py3-none-any.whl (877 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.7/877.7 kB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.22.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (23.1)\n",
      "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.3.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.3)\n",
      "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.0.1)\n",
      "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.20.1->chatgpt==0.1.0)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.20.1->chatgpt==0.1.0)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (9.0.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets->chatgpt==0.1.0)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (1.5.3)\n",
      "Collecting xxhash (from datasets->chatgpt==0.1.0)\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets->chatgpt==0.1.0)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2023.4.0)\n",
      "Collecting aiohttp (from datasets->chatgpt==0.1.0)\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19 (from datasets->chatgpt==0.1.0)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (2.0.10)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain->chatgpt==0.1.0)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain->chatgpt==0.1.0)\n",
      "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (2.8.4)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain->chatgpt==0.1.0)\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.10.7)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.2.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->chatgpt==0.1.0) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->chatgpt==0.1.0) (16.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (2.0.12)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->chatgpt==0.1.0)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->chatgpt==0.1.0)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->chatgpt==0.1.0)\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->chatgpt==0.1.0)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->chatgpt==0.1.0) (2.0.2)\n",
      "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.1.2)\n",
      "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->chatgpt==0.1.0) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2022.7.1)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.24)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.23.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.14.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->chatgpt==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (67.7.2)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.1)\n",
      "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (40.0.2)\n",
      "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n",
      "Building wheels for collected packages: chatgpt\n",
      "  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46647 sha256=36632c0034611fade439b9c8222fbc64877dd64326a47e93521eb735a55c0ed7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-u7x4upog/wheels/1a/46/ee/b32b27bbd787ce7100af8f0dfbeab0c4034eead588d448e162\n",
      "Successfully built chatgpt\n",
      "Installing collected packages: tokenizers, xxhash, mypy-extensions, multidict, marshmallow, loralib, frozenlist, dill, async-timeout, yarl, typing-inspect, responses, openapi-schema-pydantic, multiprocess, marshmallow-enum, huggingface-hub, aiosignal, transformers, dataclasses-json, aiohttp, langchain, datasets, chatgpt\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 chatgpt-0.1.0 dataclasses-json-0.5.7 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 langchain-0.0.177 loralib-0.1.1 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 responses-0.18.0 tokenizers-0.13.3 transformers-4.29.2 typing-inspect-0.8.0 xxhash-3.2.0 yarl-1.9.2\n",
      "/content/drive/MyDrive/인공지능강의\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting openai\n",
      "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.7\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting langchain==0.0.113\n",
      "  Downloading langchain-0.0.113-py3-none-any.whl (396 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.0/396.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0)\n",
      "Collecting SQLAlchemy<2,>=1 (from langchain==0.0.113)\n",
      "  Downloading SQLAlchemy-1.4.48-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (0.5.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.22.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.27.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n",
      "Installing collected packages: SQLAlchemy, langchain\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.10\n",
      "    Uninstalling SQLAlchemy-2.0.10:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.10\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.177\n",
      "    Uninstalling langchain-0.0.177:\n",
      "      Successfully uninstalled langchain-0.0.177\n",
      "Successfully installed SQLAlchemy-1.4.48 langchain-0.0.113\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers==4.28.0\n",
      "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "Successfully installed transformers-4.28.0\n"
     ]
    }
   ],
   "source": [
    "## setup(1min)\n",
    "# for ColossalAI\n",
    "!pip install colossalai==0.2.7\n",
    "\n",
    "# setup data\n",
    "\n",
    "%cd /content/drive/MyDrive/인공지능강의/GPT/colossalai_ChatGPT/\n",
    "!pip install .\n",
    "%cd ../../\n",
    "\n",
    "# setup library\n",
    "!pip install openai\n",
    "!pip install langchain==0.0.113\n",
    "!pip install pandas>=1.4.1\n",
    "!pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fti52YUFK_Gn"
   },
   "source": [
    "##Step 1) SFT: 질문에 대답을 잘하는 모델 만들기\n",
    "SFT(Supervised Fine Tuning)\n",
    "\n",
    "*   질문에 응답을 잘하도록 SFT 수행\n",
    "*   사람이 지시한 대답(13,000개), 질문에 응답(13,000개)\n",
    "* 질문-응답의 쌍으로 이루어진 데이터 셋\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDrVciWgJ-Ug"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_hLeeD7wLa--",
    "outputId": "02bd91b0-0853-4332-a417-3b74cd2fb8e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_path_1_SFT='/content/drive/MyDrive/인공지능강의/GPT/data_kochatgpt/kochatgpt_1_SFT_custom.jsonl', model_name='skt/kogpt2-base-v2', max_epochs=2, train_batch_size=8, output_dir='./output_1_SFT')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path_1_SFT', type=str, default='/content/drive/MyDrive/인공지능강의/GPT/data_kochatgpt/kochatgpt_1_SFT_custom.jsonl')\n",
    "parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "parser.add_argument('--max_epochs', type=int, default=2)\n",
    "parser.add_argument('--train_batch_size', type=int, default=8)\n",
    "parser.add_argument('--output_dir', type=str, default='./output_1_SFT')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n",
    "args.max_epochs = 2\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264,
     "referenced_widgets": [
      "24d422dd190a43d892ef1a5a66e7722a",
      "252e688e50634448a240ebebd28ad01b",
      "301ba299aff1475d9d9afd78177d09e6",
      "bb9e47400c8c46fda1e62fc7251f1583",
      "e2bdd1153c8749f891f6464adaf08f2e",
      "a4333d8a2e4a45cba39f3ce288d35e76",
      "3766369f4a474a65b7ce583750f4c8c7",
      "85ff1d8a6c284d6ead1599003f5e17d4",
      "ad11b5b99c844a1c867df64b84fd8f06",
      "54934ae1d33a4b7bb1e3739465de163d",
      "6bc86038e6184b148066aa8f9b0dd31a",
      "25c9fd4972a84a1c8121af98fb112e0a",
      "4f5df1b7b6f74983abbe37d07f82fa1c",
      "ff558a6efee54abcbb19213a901555fd",
      "4d3bde0e68a24778a7b76838aab361af",
      "82fcc0ed6a60474e93164f220ce78bc0",
      "41c02efd3fa448f2abe1ed3f87029549",
      "d486171021144d73ada7d6b32e39f3aa",
      "e96149bf9b31430ab2f04574526de38a",
      "7d1d6b6c0ff84029b7cca3281f397451",
      "401f1a8623064726a2c7b0deb5e85e26",
      "1093e7945b534ddf9e54b390767f5f91",
      "bf05565adb8b4e1bb21bc68b075d94d7",
      "35b26848b9a74c48b061e342c254f694",
      "46863496d70a4a06a87d7ade07683953",
      "21acaf8041ce429cb29cc2f98164638d",
      "e113f23aea7d45ada36459150bfcdc77",
      "9e4ae01533ef4431ac7a9dcff2f3f0ca",
      "8d8beb1fc4154be181a583f224c6752c",
      "5228819c8fa54626b65e490f022480b5",
      "4881cc743cb740749f2dd42eb77bd0a6",
      "8e1f80f6c36948329d5d0a9943db8147",
      "3b48c1ff674d4b019aef75c7dcf8e8e4"
     ]
    },
    "id": "bLM29OoULp_c",
    "outputId": "d0ed1fcb-3a8e-4f12-88ee-1a84555cbfd0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d422dd190a43d892ef1a5a66e7722a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c9fd4972a84a1c8121af98fb112e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.', '😤', ':)', 'l^o']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf05565adb8b4e1bb21bc68b075d94d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근육이 커지기 위해서는 어떻게 해야할까요?\"\n",
      "\"그렇다면 그건 바로 '내게 맞는' 방법입니다. 그리고 이 방법은 아주 간단합니다. 우선, 먼저 내 몸에 있는 지방을 제거해 줍니다. 지방세포는 우리 몸에서 가장 많이 분포돼 있습니다. 따라서 지방이 많은 사람은 자신의 몸 속에 들어있는 지방의 양을 줄여야 합니다. 그래서 나는 이것을 통해 체중을 감량하고 싶습니다. 하지만 너무 쉽게 살을 빼는 것은 좋지 않으니 주의해야겠죠. 왜냐하면 지방은 대부분 근육과 인대 등에 분포되어 있기 때문에 그만큼의 양이 많기 때문이지요. 그러므로 다이어트를 할 때는 반드시 전문의와 상의해야 하겠지요?\n",
      "이렇게 하면 살이 찌기 전에\n"
     ]
    }
   ],
   "source": [
    "## test & load skt gpt2 kroean\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "                                                    pad_token='<pad>', mask_token='<mask>')\n",
    "print(tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\"))\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "text = '근육이 커지기 위해서는 어떻게 해야할까요?'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                         max_length=128,\n",
    "                         repetition_penalty=2.0,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         bos_token_id=tokenizer.bos_token_id,\n",
    "                         use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUPS5KClwHTs",
    "outputId": "bb469301-9359-4388-f064-e2adaf1b5dc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "                                                    pad_token='<pad>', mask_token='<mask>')\n",
    "print(len(tokenizer.tokenize(\"하늘은 파란색 입니다.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GSTaz1ZHMFd",
    "outputId": "67f701ea-5de9-40d8-b715-f7180b42afd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': '0 : AI에 관심이 있니?\\n: __아직까지는 __없다\\n__;\\n'}],\n",
       " [{'generated_text': '0 : AI라는 것은 너무 재미있는거 같아\\n: 맞아 AI는 너무 흥미로워 \\n: 너도 그렇게 느끼니? 맞아 AI라는것은 너무 다양하고 신기해\\n: 괜찮아?\\n: 좋아, 좋아, 좋아.\\n: 너희들끼리 얘기하는게 재미있어\\n'}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "generator(\n",
    "    [\"0 : AI에 관심이 있니?\\n:\",\n",
    "    \"0 : AI라는 것은 너무 재미있는거 같아\\n: 맞아 AI는 너무 흥미로워 \\n: 너도 그렇게 느끼니? 맞아 AI라는것은 너무 다양하고 신기해\\n:\"],\n",
    "    **generation_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1yApLcWLquf"
   },
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Instruction(명령어):{prompt}\\n\\n###Input(입력):\\n{input}\\n\\nResponse(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Instruction(명령어):{prompt}\\n\\nResponse(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wi0Q5PPL838",
    "outputId": "b254ba2b-d2dd-4275-eeb8-6d85a0d9c58c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "## 모델 준비\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    args.model_name,\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,    \n",
    ")\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    }\n",
    ")    \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHAVJPAbL_F1",
    "outputId": "81fff875-0a89-4a30-8228-79a50680d481"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([14659, 13394, 37091, 10651,   383, 25841,  8006, 14914,  7673, 20479,\n",
      "         8091, 22311,  9036, 30902, 13675,   375,   424,  9792,   454,  9549,\n",
      "        20549,   383,  8142,  7192, 14914,   382, 37767, 13753,  8263,  7166,\n",
      "          739,  8352,  7659,  9594, 25585, 13600,  8022,  9378, 11532,  9887,\n",
      "        11218,  9111, 16691, 10351, 10561,  9128, 20479,  8091,  9065,  9446,\n",
      "         9036, 28420, 26521, 10163, 26367,  6958,  9030,  9882, 12317, 25882,\n",
      "         9209, 37194, 10351,  9036, 12168, 10529, 15989,  9719, 15434, 10552,\n",
      "        11188, 13362,  9036, 15805, 11300, 11846,  9146, 16691,  9181,  7397,\n",
      "        15806, 13480, 11342, 17596,  9161, 19996,  9025, 25006, 18595,  9966,\n",
      "        12592, 10751, 11814,  8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,   382, 37767, 13753,  8263,  7166,\n",
      "          739,  8352,  7659,  9594, 25585, 13600,  8022,  9378, 11532,  9887,\n",
      "        11218,  9111, 16691, 10351, 10561,  9128, 20479,  8091,  9065,  9446,\n",
      "         9036, 28420, 26521, 10163, 26367,  6958,  9030,  9882, 12317, 25882,\n",
      "         9209, 37194, 10351,  9036, 12168, 10529, 15989,  9719, 15434, 10552,\n",
      "        11188, 13362,  9036, 15805, 11300, 11846,  9146, 16691,  9181,  7397,\n",
      "        15806, 13480, 11342, 17596,  9161, 19996,  9025, 25006, 18595,  9966,\n",
      "        12592, 10751, 11814,  8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "## prepare data\n",
    "from typing import Optional, Dict, Sequence\n",
    "    \n",
    "class SFT_dataset(Dataset):\n",
    "    '''SFT dataset by wygo'''\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        \n",
    "        ## format\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_input = 'input'  \n",
    "        pattern_output = 'completion'  \n",
    "\n",
    "        ############################################################\n",
    "        ## load dataset\n",
    "\n",
    "        data_path_1_SFT = '/content/drive/MyDrive/인공지능강의/GPT/data_kochatgpt/kochatgpt_1_SFT_custom.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "            if verbose:\n",
    "                print('## data check ##')\n",
    "                print((list_data_dict[0]))\n",
    "\n",
    "        ############################################################\n",
    "        ## 데이터셋 만들기, source와 target\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기\n",
    "\n",
    "        # 입력\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if example.get(pattern_input, \"\") != \"\":\n",
    "                tmp = prompt_input.format_map(example)\n",
    "            else:\n",
    "                tmp = prompt_no_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        # 출력\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "\n",
    "        if verbose:\n",
    "            idx = 0\n",
    "            print((sources[idx]))\n",
    "            print((targets[idx]))\n",
    "            print(\"Tokenizing inputs... This may take some time...\")\n",
    "\n",
    "        ############################################################\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        # source data tokenized\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source만\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "\n",
    "        ## 입력은 source, 출력은 source+target 이지만 학습은 target 부분만\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = IGNORE_INDEX  # source 부분은 -100으로 채운다\n",
    "        \n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)        \n",
    "        \n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))    \n",
    "        \n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        \"\"\"Tokenize a list of strings.\"\"\"\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n",
    "eval_dataset  = None  # eval은 안함\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "# check\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3wy8oGVgi_4",
    "outputId": "580ef79b-3be6-4dab-b2d2-91811bfdceed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.19.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "Vi1iZ5VXMo9v",
    "outputId": "950e626b-ca62-47cf-e196-2962e51fe157"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3001' max='3001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3001/3001 14:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.952300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.849600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.664700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 학습 (10min)\n",
    "# training_args 수정 가능: https://github.com/Beomi/KoAlpaca/blob/main/train.sh 참고\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/인공지능강의/GPT\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=1, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    eval_steps = 3, # Number of update steps between two evaluations.\n",
    "    save_steps=500, # after # steps model is saved \n",
    "    warmup_steps=5,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_state()\n",
    "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPct4AxNM_gA"
   },
   "source": [
    "##GPT2모델이 사람의 질문에 대해 대답하는 모델을 학습함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_Ni7ztKM2Id",
    "outputId": "2abac628-8598-4a6e-ffc5-8348065ffdf4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Chat AI를 실행합니다.-----------------\n",
      "\n",
      "질문 : Instruction(명령어):AI란 무엇인가?\n",
      "\n",
      "Response(응답):'AI란 인공지능 어시스턴트이기 때문에 인간과 같은 감정을 느끼지는 않습니다. 하지만 AI는 다양한 분야에서 활용될 수 있습니다. 예를 들어, 로봇, 컴퓨터, 스마트폰, PDF 파일 등 다양한 분야에서 활용됩니다. AI를 통해 인간과는 다른 경험을 할 수 있습니다. AI\n",
      "\n",
      "\n",
      "-----------------Chat AI를 실행합니다.-----------------\n",
      "\n",
      "질문 : Instruction(명령어):강원도는 무엇이 유명한가요?\n",
      "\n",
      "Response(응답):'강원도에는 다양한 관광지가 있습니다. 대표적인 관광지로는 영월, 삼척, 속초, 고성 등이 있습니다. 또한 강릉과 속초도 대표적인 관광지 중 하나입니다. 高敞, 固城, 江原, 永川 등이 있습니다. 高敞은 조선 시대 문신인 노자규(李子圭)가 지은\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 추론 테스트\n",
    "generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)\n",
    "# generator = pipeline('text-generation', model=model.cpu(), tokenizer=tokenizer, config={'max_length':800})\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0, #반복되는 텍스트를 억제함\n",
    "    no_repeat_ngram_size=4, #개의 연속적인 단어를 반복하지 않는다.\n",
    "    eos_token_id=375, #EOS(End-of-Sequence)를 나타내는 토큰 ID를 지정 및 텍스트를 생성할 때 모델은 이 EOS 토큰에 도달하면 토큰 생성을 중지\n",
    "    max_new_tokens=64,#생성하는 토큰의 최대 수\n",
    "    do_sample=True, #True로 설정하면 모델이 예측 확률에 따라 다음 토큰을 무작위 선택 \n",
    "    top_k=50, #다음 토큰을 예측할때 예측 확률이 가장 높은 상위 k 개로 예측함\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "list_prompt = [\"AI란 무엇인가?\", \"강원도는 무엇이 유명한가요?\"]\n",
    "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print((\"-----------------Chat AI를 실행합니다.-----------------\\n\"))\n",
    "    print(('질문 : {}\\n\\n').format(result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLGRyTPpNMQX"
   },
   "source": [
    "##Step 2) RM : 좋은 글 채점기 만들기\n",
    "\n",
    "* 생성된 각 글에 대한 점수를 매기고 이를 보상모델로 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9-DqeHDg42G",
    "outputId": "75f229e1-183b-4ef0-98de-f6f54ed96117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: colossalai==0.2.7 in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.22.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.65.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (23.1)\n",
      "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.3.2)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.3.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.3)\n",
      "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.0.1)\n",
      "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (0.3.3)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.11.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (2.0.1+cu118)\n",
      "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (2.1.2)\n",
      "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (3.1.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (3.3.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (2.5.24)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (1.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (20.23.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->colossalai==0.2.7) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->colossalai==0.2.7) (16.0.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (67.7.2)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (4.0.1)\n",
      "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (40.0.2)\n",
      "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (1.5.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->colossalai==0.2.7) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->colossalai==0.2.7) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torch==1.13.1\n",
      "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.5.0)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.40.0)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1+cu118\n",
      "    Uninstalling torch-2.0.1+cu118:\n",
      "      Successfully uninstalled torch-2.0.1+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n",
      "/content/drive/MyDrive/인공지능강의/GPT/colossalai_ChatGPT\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Processing /content/drive/MyDrive/인공지능강의/GPT/colossalai_ChatGPT\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: transformers>=4.20.1 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.28.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.65.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.12.0)\n",
      "Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.1.1)\n",
      "Requirement already satisfied: colossalai>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.2.7)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (1.13.1)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.0.113)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.22.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (23.1)\n",
      "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.3.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.3)\n",
      "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.0.1)\n",
      "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.14.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.13.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.18.0)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.4.48)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (0.5.7)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.10.7)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.2.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->chatgpt==0.1.0) (67.7.2)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->chatgpt==0.1.0) (0.40.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain->chatgpt==0.1.0) (2.0.2)\n",
      "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.1.2)\n",
      "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2022.7.1)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.24)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.23.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.1)\n",
      "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (40.0.2)\n",
      "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.0.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n",
      "Building wheels for collected packages: chatgpt\n",
      "  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46647 sha256=0eb6e9a0763eda157c6e9957de21d23dd3bb734b4ad71333835960cba5d8f9ce\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gs324lkb/wheels/1a/46/ee/b32b27bbd787ce7100af8f0dfbeab0c4034eead588d448e162\n",
      "Successfully built chatgpt\n",
      "Installing collected packages: chatgpt\n",
      "  Attempting uninstall: chatgpt\n",
      "    Found existing installation: chatgpt 0.1.0\n",
      "    Uninstalling chatgpt-0.1.0:\n",
      "      Successfully uninstalled chatgpt-0.1.0\n",
      "Successfully installed chatgpt-0.1.0\n",
      "/content/drive/MyDrive/인공지능강의\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.7)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: langchain==0.0.113 in /usr/local/lib/python3.10/dist-packages (0.0.113)\n",
      "Requirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.4.48)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.8.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (0.5.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.22.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.27.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "## setup(1min)\n",
    "# for ColossalAI\n",
    "!pip install colossalai==0.2.7\n",
    "!pip install torch==1.13.1\n",
    "# setup data\n",
    "\n",
    "%cd /content/drive/MyDrive/인공지능강의/GPT/colossalai_ChatGPT/\n",
    "!pip install .\n",
    "%cd ../../\n",
    "\n",
    "# setup library\n",
    "!pip install openai\n",
    "!pip install langchain==0.0.113\n",
    "!pip install pandas>=1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mZJmQZiNQ2G"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import loralib as lora\n",
    "import torch\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.bloom import BLOOMRM\n",
    "from chatgpt.models.gpt import GPTRM\n",
    "from chatgpt.models.opt import OPTRM\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Instruction(명령어):{prompt}\\n\\n### Input(입력):\\n{input}\\n\\n Response(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Instruction(명령어):{prompt}\\n\\n Response(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlYLhC-7NmuW"
   },
   "outputs": [],
   "source": [
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--output_dir', type=str, default='./output_2_RM')\n",
    "parser.add_argument('--data_path_2_RM', type=str, default='/content/drive/MyDrive/인공지능강의/GPT/data_kochatgpt/kochatgpt_2_RM_custom.jsonl')\n",
    "parser.add_argument('--strategy',\n",
    "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
    "                    default='naive')\n",
    "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "parser.add_argument('--pretrain', type=str, default=None)\n",
    "parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n",
    "parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n",
    "parser.add_argument('--max_epochs', type=int, default=10)\n",
    "parser.add_argument('--batch_size', type=int, default=4)\n",
    "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
    "parser.add_argument('--max_len', type=int, default=512)  # wygo 추가\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.max_epochs = 3\n",
    "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained 모델 가져오기\n",
    "args.verbose = True\n",
    "\n",
    "print(args)\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4C7fHvYGOGg9"
   },
   "outputs": [],
   "source": [
    "# configure strategy\n",
    "if args.strategy == 'naive':\n",
    "    strategy = NaiveStrategy()\n",
    "elif args.strategy == 'ddp':\n",
    "    strategy = DDPStrategy()\n",
    "elif args.strategy == 'colossalai_gemini':\n",
    "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
    "elif args.strategy == 'colossalai_zero2':\n",
    "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgSF1K2uOLNU"
   },
   "outputs": [],
   "source": [
    "# customizing, https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/nn/gpt_rm.py#L29\n",
    "from typing import Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "# from ..base import RewardModel\n",
    "from chatgpt.models.base import RewardModel\n",
    "\n",
    "\n",
    "class GPTRM_custom(RewardModel):\n",
    "    \"\"\"\n",
    "    GPT Reward model.\n",
    "    Args:\n",
    "        pretrained (str): Pretrained model name or path.\n",
    "        config (GPT2Config): Model config.\n",
    "        checkpoint (bool): Enable gradient checkpointing.\n",
    "        lora_rank (int): Rank of the low-rank approximation.\n",
    "        lora_train_bias (str): LoRA bias training mode.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))  # wygo 추가!!!\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        # model = model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab2NqplAOPOU"
   },
   "outputs": [],
   "source": [
    "# configure model, tokenizer\n",
    "with strategy.model_init_context():\n",
    "    # load pretrained gpt2    \n",
    "    if args.model == 'gpt2':\n",
    "#         tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(args.pretrain)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = GPTRM_custom(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n",
    "\n",
    "    elif args.model == 'bloom':\n",
    "        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
    "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
    "    \n",
    "    elif args.model == 'opt':\n",
    "        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")      \n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
    "    \n",
    "    \n",
    "    # model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lr2hPwPFOS6V"
   },
   "outputs": [],
   "source": [
    "# make ranking data to chosen, rejetced data\n",
    "with open(args.data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    if args.verbose:\n",
    "        print('## data check ##')\n",
    "        print((list_data_dict[0]))\n",
    "        \n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    # data 1) 0 VS 1\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "    # data 2) 0 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    # data 1) 1 VS 2\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85ca93LKB5xN"
   },
   "outputs": [],
   "source": [
    "len(total_data_ranking2chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kv358luNOX8W"
   },
   "outputs": [],
   "source": [
    "# prepare for data and dataset\n",
    "import random\n",
    "random.seed(230319)\n",
    "# list_tmp = list(range(10))\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])\n",
    "\n",
    "# train_data = total_data_ranking2chosen[:-1000]  # 29000 학습\n",
    "# eval_data = total_data_ranking2chosen[-1000:0]  # 1000개만 평가\n",
    "\n",
    "train_data = total_data_ranking2chosen[:100]  # 학습 데이터의 수\n",
    "eval_data = total_data_ranking2chosen[100:130]  # 평가 데이터의 수\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
    "\n",
    "# check\n",
    "idx = 10\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rq99mcy2OjIT"
   },
   "outputs": [],
   "source": [
    "# configure optimizer\n",
    "if args.strategy.startswith('colossalai'):\n",
    "    optim = HybridAdam(model.parameters(), lr=5e-5)\n",
    "else:\n",
    "    optim = Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ua2vKRgOkjr"
   },
   "outputs": [],
   "source": [
    "# batch_size here is expected to be C(k,2), k means # response of each prompt\n",
    "# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n",
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=strategy,\n",
    "                             optim=optim,\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=args.batch_size,\n",
    "                             max_epochs=args.max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m25-9pd7Oq3d"
   },
   "source": [
    "##RM을 통해 각 순위에 대해 점수를 부여하고 가장 정확한 응답에 대해 학습함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5DeYx6jOmWv"
   },
   "outputs": [],
   "source": [
    "# train!!\n",
    "trainer.fit(use_lora=args.lora_rank)\n",
    "\n",
    "## save\n",
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(optim,\n",
    "                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vFmdP46P3cz"
   },
   "source": [
    "## Step 3) PPO 사람의 피드백을 반영하여 질문에 대한 응답이 피드백과 비슷해 지도록 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_vRtPXodlmf"
   },
   "outputs": [],
   "source": [
    "## setup(1min)\n",
    "# for ColossalAI\n",
    "!pip install colossalai==0.2.7\n",
    "!pip install torch==1.13.1\n",
    "# setup data\n",
    "\n",
    "%cd /content/drive/MyDrive/인공지능강의/GPT/colossalai_ChatGPT/\n",
    "!pip install .\n",
    "%cd ../../\n",
    "\n",
    "# setup library\n",
    "!pip install openai\n",
    "!pip install langchain==0.0.113\n",
    "!pip install pandas>=1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTa3nVDuO5Nk"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.models.opt import OPTActor, OPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "from colossalai.nn.optimizer import HybridAdam\n",
    "\n",
    "## wy 추가\n",
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "## clossalAI error 해결\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '2'\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '42043'\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Instruction(명령어):{prompt}\\n\\n### Input(입력):\\n{input}\\n\\nResponse(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Instruction(명령어):{prompt}\\n\\nResponse(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzLnJEHZPB2T"
   },
   "outputs": [],
   "source": [
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path_3_PPO', type=str, default='/content/drive/MyDrive/인공지능강의/GPT/data_kochatgpt/kochatgpt_3_PPO.jsonl')\n",
    "parser.add_argument('--output_dir', type=str, default='./output_3_PPO')\n",
    "parser.add_argument('--strategy',\n",
    "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
    "                    default='naive')\n",
    "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "parser.add_argument('--pretrain', type=str, default=None)\n",
    "parser.add_argument('--num_episodes', type=int, default=10)\n",
    "parser.add_argument('--max_timesteps', type=int, default=3)\n",
    "parser.add_argument('--update_timesteps', type=int, default=3)\n",
    "parser.add_argument('--max_epochs', type=int, default=5)\n",
    "parser.add_argument('--train_batch_size', type=int, default=8)\n",
    "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
    "parser.add_argument('--max_length', type=int, default=250)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.output_dir = './output_3_PPO'\n",
    "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained 모델 가져오기\n",
    "\n",
    "# args.pretrain_actor = './output_1_SFT'  # SFT 모델 가져오기\n",
    "# args.pretrain_critic = './output_2_RM'  # RM 모델 가져오기\n",
    "args.pretrain_actor = args.pretrain\n",
    "args.pretrain_critic = args.pretrain\n",
    "\n",
    "args.num_episodes = 1\n",
    "args.max_epochs   = 1\n",
    "\n",
    "print(args)\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pisr96oIccRT"
   },
   "outputs": [],
   "source": [
    "args.pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9xRGXwiPNs3"
   },
   "outputs": [],
   "source": [
    "# configure strategy\n",
    "if args.strategy == 'naive':\n",
    "    strategy = NaiveStrategy()\n",
    "elif args.strategy == 'ddp':\n",
    "    strategy = DDPStrategy()\n",
    "elif args.strategy == 'colossalai_gemini':\n",
    "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
    "elif args.strategy == 'colossalai_zero2':\n",
    "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og7ueBVvPPtH"
   },
   "outputs": [],
   "source": [
    "# configure model, tokenizer\n",
    "with strategy.model_init_context():\n",
    "    if args.model == 'gpt2':\n",
    "        actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )    \n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "    elif args.model == 'bloom':\n",
    "        actor = BLOOMActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        critic = BLOOMCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
    "        tokenizer.pad_token = tokenizer.eos_token            \n",
    "    elif args.model == 'opt':\n",
    "        actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        critic = OPTCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")            \n",
    "    else:\n",
    "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DhMVvYjPR2n"
   },
   "outputs": [],
   "source": [
    "# configure optimizer\n",
    "if args.strategy.startswith('colossalai'):\n",
    "    actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n",
    "    critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n",
    "else:\n",
    "    actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "    critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sp_dgLebPTe3"
   },
   "outputs": [],
   "source": [
    "# setting the models\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThJWdu2aPVD-"
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "print(list_prompt)\n",
    "print('\\n\\n\\n')\n",
    "print(tokenize_fn('I want you to act as a linux terminal.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mctryMQGPV6y"
   },
   "outputs": [],
   "source": [
    "# configure trainer\n",
    "trainer = PPOTrainer(strategy,\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=args.max_epochs,\n",
    "                     train_batch_size=args.train_batch_size,\n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "## train!\n",
    "trainer.fit(list_prompt,  # 입력 prompt\n",
    "            num_episodes=args.num_episodes,\n",
    "            max_timesteps=args.max_timesteps,\n",
    "            update_timesteps=args.update_timesteps)\n",
    "\n",
    "## save\n",
    "# save model checkpoint after fitting on only rank0\n",
    "strategy.save_model(actor, os.path.join(args.output_dir, 'actor.pt'), only_rank0=True)\n",
    "# save optimizer checkpoint on all ranks\n",
    "strategy.save_optimizer(actor_optim,\n",
    "                        os.path.join(args.output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
    "                        only_rank0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ydF3RbQE0Uy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXGgCwz5KH01"
   },
   "outputs": [],
   "source": [
    "actor=GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ahSVzSxPZdS"
   },
   "outputs": [],
   "source": [
    "## inference\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=args.max_length,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print(\"-\"*70)\n",
    "    print((\"챗봇 : {}\").format(output))\n",
    "    print(\"-\"*70)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ldehpLuJEk_"
   },
   "outputs": [],
   "source": [
    "list_prompt = [\"인공지능은 무엇인가 설명해줘\",\"하늘에는 왜 구름이 있을까?\"]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "  print(\"-\"*70)\n",
    "  print((\"질문 : {}\").format(input_text))\n",
    "  output = generation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QA-sQ0ig5M2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h16qOhQug5J_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdUS_YHlhXeu"
   },
   "source": [
    "##학습 데이터 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgXMqdh5g5Hu"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from chatgpt.models.bloom import BLOOMActor\n",
    "from chatgpt.models.gpt import GPTActor\n",
    "from chatgpt.models.opt import OPTActor\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "\n",
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Instruction(명령어):{prompt}\\n\\n### Input(입력):\\n{input}\\n\\nResponse(응답):\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Instruction(명령어):{prompt}\\n\\nResponse(응답):\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ynju55ZIg7_t",
    "outputId": "cf77da63-1b18-4b82-a1c9-16a625a8b64d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# define argment\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model',\n",
    "                    default='gpt2',\n",
    "                    choices=['gpt2', 'bloom', 'opt'])\n",
    "# We suggest to use the pretrained model from HuggingFace, use pretrain to configure model\n",
    "parser.add_argument('--pretrain', type=str, default=None)\n",
    "parser.add_argument('--model_path', type=str, default=None)\n",
    "parser.add_argument('--input',\n",
    "                    type=str,\n",
    "                    default='Question: How are you ? Answer:')\n",
    "parser.add_argument('--max_length', type=int, default=250)\n",
    "args_inference = parser.parse_args([])\n",
    "\n",
    "args_inference.model = 'gpt2'\n",
    "args_inference.pretrain = 'skt/kogpt2-base-v2'\n",
    "args_inference.model_directory = '/content/drive/MyDrive/인공지능강의/output_3_PPO'\n",
    "args_inference.model_path = os.path.join(args_inference.model_directory, 'actor.pt')\n",
    "\n",
    "# configure model, tokenizer\n",
    "if args_inference.model == 'gpt2':\n",
    "    #actor = GPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
    "    actor = GPTActor(pretrained=args_inference.pretrain)\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained(args_inference.pretrain)\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args_inference.pretrain,\n",
    "                                              padding_side=\"right\",\n",
    "                                              model_max_length=512)\n",
    "    tokenizer.add_special_tokens({\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    })\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "elif args_inference.model == 'bloom':\n",
    "    actor = BLOOMActor(pretrained=args_inference.pretrain).to(\n",
    "        torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "elif args_inference.model == 'opt':\n",
    "    actor = OPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported model \"{args_inference.model}\"')\n",
    "\n",
    "state_dict = torch.load(args_inference.model_path, map_location='cpu');\n",
    "actor.model.load_state_dict(state_dict);\n",
    "\n",
    "actor.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XGTiOOJhLaG"
   },
   "outputs": [],
   "source": [
    "## inference\n",
    "def generation(input_text):\n",
    "    #input_ids = tokenizer.encode(input_text, return_tensors='pt').to(torch.cuda.current_device())\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=args_inference.max_length,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print(\"-\"*70)\n",
    "    print((\"챗봇 : {}\").format(output))\n",
    "    print(\"-\"*70)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5P4Qp1hqhNPW",
    "outputId": "2f0fbabf-82b3-4db2-d6fb-bf49f10bfebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "질문 : 인공지능은 무엇인가 설명해줘\n",
      "----------------------------------------------------------------------\n",
      "챗봇 : 인공지능은 무엇인가 설명해줘요.\n",
      "어머님이 보시는 게 바로 그거예요?\n",
      "어머님은 그게 뭔지 아세요?\n",
      "아니 그냥 그~ 알 수가 없는 건데 왜 그~ 그~ 그~ 봇물 터지는 그~ 영상을 보고 진짜 어머님은 얼마나 이쁘게 보셨을까라고 생각하시겠지만 그~ 그~ 그~ 그~ 어머님이 이게 무슨 소릴 하시는지 몰랐지만 이제 그~ 뭐랄까 되게 궁금하잖아요.\n",
      "그~ 그~ 그~ 아~ 그~ 그~ 그~ 그~ 니까?\n",
      "아~ 그래서 그~ 아~ 제가 그~ 지금 그~ 니까?\n",
      "그래서 그~ 그~ 니까?\n",
      "그~ 아~ 아니 근데 아니 그~ 니까?\n",
      "그~ 그~ 아~ 그럼 어~ 뭐야 아~ 그~ 아~ 어~ 이~ 그~ 뭔지 궁금하잖아요.\n",
      "그~ 니?\n",
      "그~ 그~ 니까?\n",
      "그~ 아~ 뭔지 궁금한 거예요?\n",
      "아니 니까?\n",
      "니까?\n",
      "아니 뭐야 그래서 니까?\n",
      "아니 그~ 저희 아~ 그~\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "질문 : 하늘에는 왜 구름이 있을까?\n",
      "----------------------------------------------------------------------\n",
      "챗봇 : 하늘에는 왜 구름이 있을까? 어때? 지금이 땡땡이이야. 우리 집이 다 좋았지.\"\n",
      "그 말에 놀란 엄마는 말했다.\n",
      "\"나는 다들 좋았어. 난 엄마한테 잘됐어. 너희들 다 잘됐어. 내 아들도 엄마가 잘됐으면 좋겠다고. 우리 집 사람들이 잘돼서 좋았어.\"\n",
      "\"그렇다면 이제 집으로 돌아가도 돼?\"\n",
      "\"내가 엄마한테 잘됐으면 좋겠어.\"\n",
      "\"그럼, 우리 집 사람들에게 잘될 거야. 우리 집 사람들은 다 잘됐어. 내 아들도 잘됐으면 좋겠어.\"\n",
      "\"이렇게 말했거든. 내가 엄마한테 잘됐으면 좋겠어. 내 아들도 잘됐으면 좋겠어.\"\n",
      "\"엄마. 우리 집 사람들 다 잘됐으면 좋겠어. 엄마는 땡땡이도 잘됐으면 좋겠어.\"\n",
      "\"그래. 우리 집 사람들에게 잘됐어. 우리 동네 사람들은 다 잘됐으면 좋겠어.\"\n",
      "\"그럼, 엄마. 이게 엄마가 잘됐으면 좋겠어. 엄마가 땡땡이를 많이 잘했으면 좋겠어. 엄마는 다 잘됐으면 좋겠어.\"\n",
      "\"그럼 내가 너\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "list_prompt = [\"인공지능은 무엇인가 설명해줘\",\"하늘에는 왜 구름이 있을까?\"]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "  print(\"-\"*70)\n",
    "  print((\"질문 : {}\").format(input_text))\n",
    "  output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKFjohc4OGLk"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
